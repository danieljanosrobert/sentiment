{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in c:\\users\\daniel_janos_robert\\anaconda3\\envs\\bow\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\daniel_janos_robert\\anaconda3\\envs\\bow\\lib\\site-packages (from tensorflow-addons) (2.12.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Name: numpy\n",
      "Version: 1.19.2\n",
      "Summary: NumPy is the fundamental package for array computing with Python.\n",
      "Home-page: https://www.numpy.org\n",
      "Author: Travis E. Oliphant et al.\n",
      "Author-email: None\n",
      "License: BSD\n",
      "Location: c:\\users\\daniel_janos_robert\\anaconda3\\envs\\bow\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: tensorflow, tensorboard, seaborn, scipy, scikit-learn, pandas, opt-einsum, mkl-random, mkl-fft, matplotlib, Keras, Keras-Preprocessing, Keras-Applications, h5py\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "%pip install -U tensorflow-addons\n",
    "%pip show numpy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "LABELS = ['positive', 'neutral', 'negative']\n",
    "\n",
    "SAVE_MODEL = True\n",
    "DUMP_DIRECTORY = 'created_models'\n",
    "\n",
    "TRAIN_RANDOM_STATE = 42\n",
    "DEV_RANDOM_STATE = 42\n",
    "\n",
    "DROP=0.2\n",
    "LAYER_1_DENSITY = 1024\n",
    "LAYER_2_DENSITY = 512\n",
    "LAYER_3_DENSITY = 256\n",
    "ACTIVATION = 'sigmoid'\n",
    "\n",
    "EMBEDDING_DIMENSIONS = 100\n",
    "LAST_DIMENSIONS = 3 #based on labels\n",
    "LAST_ACTIVATION = 'softmax'\n",
    "MAX_LENGTH = 1000\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 600\n",
    "COMPILE_LOSS = 'categorical_crossentropy'\n",
    "COMPILE_METRICS = ['accuracy']\n",
    "\n",
    "GLOVE_DIMENSIONS = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "def dump_file(o, object_name, name):\n",
    "    if not os.path.exists(DUMP_DIRECTORY):\n",
    "        os.mkdir(DUMP_DIRECTORY)\n",
    "    time = datetime.now().strftime(\"%d%b%Y%H%M%S\")\n",
    "    created_model_path = DUMP_DIRECTORY + '/' + name + '_' + time + '.dump'\n",
    "    joblib.dump(o, created_model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6275 entries, 5821 to 7270\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   sentiment   6275 non-null   object\n",
      " 1   tweet_text  6275 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 147.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1051 entries, 65 to 1126\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   sentiment   1051 non-null   object\n",
      " 1   tweet_text  1051 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 24.6+ KB\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "HEADER = ['id1', 'id2', 'sentiment', 'tweet_text']\n",
    "HEADER_TO_DELETE = ['id1', 'id2']\n",
    "\n",
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split()\n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1]\n",
    "    return \" \".join(clean_words)\n",
    "\n",
    "def remove_mentions(input_text):\n",
    "    URL_RE = 'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}([-a-zA-Z0-9()@:%_+.~#?&/=]*)'\n",
    "    input_text = re.sub(URL_RE, '', input_text)\n",
    "    input_text = re.sub(r'@\\w+', '', input_text)\n",
    "    return re.sub(r'#\\w+', '', input_text)\n",
    "\n",
    "def clean(dataset, col='tweet_text', not_equals_text='Not Available'):\n",
    "    return dataset[dataset[col] != not_equals_text]\n",
    "\n",
    "def merge_neutrals(dataset):\n",
    "    neutral_sentiments = ['objective', 'objective-OR-neutral']\n",
    "    dataset['sentiment'] = dataset['sentiment'].apply(lambda x: 'neutral' if x in neutral_sentiments else x)\n",
    "    return dataset\n",
    "\n",
    "def evaluate(predict, labels):\n",
    "    print('Classification report:')\n",
    "    print(classification_report(labels, predict))\n",
    "    print('Accuracy:')\n",
    "    print(accuracy_score(labels, predict))\n",
    "\n",
    "    print('Confusion matrix:')\n",
    "    df_cm = pd.DataFrame(confusion_matrix(labels, predict),\n",
    "                         index=[i for i in ['positive', 'neutral', 'negative']],\n",
    "                         columns=[i for i in ['positive', 'neutral', 'negative']])\n",
    "    plt.figure(figsize=(10,7))\n",
    "    hm = sn.heatmap(df_cm, annot=True, fmt='g', cmap=\"Blues\")\n",
    "    hm.set(ylabel='True label', xlabel='Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "train_dataset = pd.read_csv('db/train.tsv', sep='\\t', header=None, names=HEADER)\n",
    "train_dataset = shuffle(train_dataset, random_state=TRAIN_RANDOM_STATE)\n",
    "train_dataset = clean(train_dataset)\n",
    "train_dataset = merge_neutrals(train_dataset)\n",
    "train_dataset.drop(HEADER_TO_DELETE, axis=1, inplace=True)\n",
    "train_dataset['tweet_text'] = train_dataset['tweet_text'].apply(remove_stopwords).apply(remove_mentions)\n",
    "train_dataset.info()\n",
    "\n",
    "dev_dataset = pd.read_csv('db/dev-full.tsv', sep='\\t', header=None, names=HEADER)\n",
    "dev_dataset = shuffle(dev_dataset, random_state=DEV_RANDOM_STATE)\n",
    "dev_dataset = clean(dev_dataset)\n",
    "dev_dataset = merge_neutrals(dev_dataset)\n",
    "dev_dataset.drop(HEADER_TO_DELETE, axis=1, inplace=True)\n",
    "dev_dataset['tweet_text'] = dev_dataset['tweet_text'].apply(remove_stopwords).apply(remove_mentions)\n",
    "dev_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     sentiment                                         tweet_text\n5821   neutral                  I'm bout listen nicki minaj night\n2833  positive   see C. Edwards anything racing hard Thurs Due...\n2543  positive   fux wit yo 3rd choice Gifted Hands (the Ben C...\n1737  positive  First time listing Red: \"Does diva think Avril...\n6582  positive   Congrats Lloyd Robertson Gordon Sinclair Awar...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>tweet_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5821</th>\n      <td>neutral</td>\n      <td>I'm bout listen nicki minaj night</td>\n    </tr>\n    <tr>\n      <th>2833</th>\n      <td>positive</td>\n      <td>see C. Edwards anything racing hard Thurs Due...</td>\n    </tr>\n    <tr>\n      <th>2543</th>\n      <td>positive</td>\n      <td>fux wit yo 3rd choice Gifted Hands (the Ben C...</td>\n    </tr>\n    <tr>\n      <th>1737</th>\n      <td>positive</td>\n      <td>First time listing Red: \"Does diva think Avril...</td>\n    </tr>\n    <tr>\n      <th>6582</th>\n      <td>positive</td>\n      <td>Congrats Lloyd Robertson Gordon Sinclair Awar...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "def evaluate(predict, labels, history):\n",
    "    print('Classification report:')\n",
    "    print(classification_report(labels, predict))\n",
    "    print('Accuracy:')\n",
    "    print(accuracy_score(labels, predict))\n",
    "\n",
    "    print('Confusion matrix:')\n",
    "    df_cm = pd.DataFrame(confusion_matrix(labels, predict),\n",
    "                         index=[i for i in ['positive', 'neutral', 'negative']],\n",
    "                         columns=[i for i in ['positive', 'neutral', 'negative']])\n",
    "    plt.figure(figsize=(10,7))\n",
    "    hm = sn.heatmap(df_cm, annot=True, fmt='g', cmap=\"Blues\")\n",
    "    hm.set(ylabel='True label', xlabel='Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train acc', 'val acc'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train loss', 'val loss'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "x_train = train_dataset['tweet_text'].values\n",
    "y_train = train_dataset['sentiment'].values\n",
    "\n",
    "x_dev = dev_dataset['tweet_text'].values\n",
    "y_dev = dev_dataset['sentiment'].values\n",
    "\n",
    "train_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# tk = Tokenizer(num_words=NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{\"}~\\t\\n',\n",
    "#               lower=True, split=\" \")\n",
    "\n",
    "tk = Tokenizer()\n",
    "all_tweets = np.append(x_train, x_dev)\n",
    "tk.fit_on_texts(all_tweets)\n",
    "\n",
    "vocab_size = len(tk.word_index) + 1\n",
    "\n",
    "x_train_seq = tk.texts_to_sequences(x_train)\n",
    "x_dev_seq = tk.texts_to_sequences(x_dev)\n",
    "\n",
    "x_train_seq_trunc = pad_sequences(x_train_seq, maxlen=MAX_LENGTH, padding='post')\n",
    "x_dev_seq_trunc = pad_sequences(x_dev_seq, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_le = le.fit_transform(y_train)\n",
    "y_dev_le = le.fit_transform(y_dev)\n",
    "y_train_categorical = to_categorical(y_train_le)\n",
    "y_dev_categorical = to_categorical(y_dev_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 1000, 100)         1510900   \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 1,756,023\n",
      "Trainable params: 1,756,023\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def create_model(embedding_layer):\n",
    "    sequence_input = models.Input(shape=(MAX_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = layers.Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "    x = layers.MaxPooling1D(5)(x)\n",
    "    x = layers.Conv1D(128, 5, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(5)(x)\n",
    "    x = layers.Conv1D(128, 5, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(35)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    preds = layers.Dense(LAST_DIMENSIONS, activation='softmax')(x)\n",
    "    return sequence_input, preds\n",
    "\n",
    "embedding_layer = layers.Embedding(vocab_size, GLOVE_DIMENSIONS, input_length=max_length)\n",
    "sequence_input, preds = create_model(embedding_layer)\n",
    "\n",
    "emb_model = models.Model(sequence_input, preds)\n",
    "emb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tqdm_callback = tfa.callbacks.TQDMProgressBar()\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "\n",
    "def deep_model(model, x_train, y_train, x_dev, y_dev):\n",
    "    model.compile(loss=COMPILE_LOSS, metrics=COMPILE_METRICS, optimizer='adam')\n",
    "\n",
    "    history = model.fit(x_train, y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n",
    "                        validation_data=(x_dev, y_dev), verbose=0, callbacks=[EarlyStopping(patience=2), tqdm_callback])\n",
    "    # TODO: shuffle -> random state, with seeds\n",
    "    # if SAVE_MODEL:\n",
    "    #   dump_file(model, 'neural net', 'deep_model')\n",
    "\n",
    "    result = model.evaluate(x_dev, y_dev)\n",
    "    predict = model.predict_classes(x_dev)\n",
    "\n",
    "    return history, result, predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Training:   0%|           0/50 ETA: ?s,  ?epochs/s",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17d9c0b7616540a7abf47750a71ef937"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Epoch 2/50\n",
      "Epoch 3/50\n",
      "Epoch 4/50\n",
      "Epoch 5/50\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 1.3427 - accuracy: 0.4634\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": "0/11           ETA: ?s - ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "431baeb58bd443709ca7b191432ca388"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0/11           ETA: ?s - ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58f59876b48642d19d474f362ff4158b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0/11           ETA: ?s - ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2e5e65b9d624fa0a42f72bf8e476925"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0/11           ETA: ?s - ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69c6d57f881643e1a6c06680960d82e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0/11           ETA: ?s - ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c213d6f372824fdfaf2c88456f1a9ce9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0/11           ETA: ?s - ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89eb17e5486e4642b698c98f968399b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-26-8603ce837cdb>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m emb_history, emb_result, emb_predict = deep_model(emb_model, x_train_seq_trunc, y_train_categorical,\n\u001B[0m\u001B[0;32m      2\u001B[0m                          x_dev_seq_trunc, y_dev_categorical)\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-25-c6b7ae450b2b>\u001B[0m in \u001B[0;36mdeep_model\u001B[1;34m(model, x_train, y_train, x_dev, y_dev)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mevaluate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_dev\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_dev\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m     \u001B[0mpredict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict_classes\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_dev\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mhistory\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpredict\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Functional' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "emb_history, emb_result, emb_predict = deep_model(emb_model, x_train_seq_trunc, y_train_categorical,\n",
    "                         x_dev_seq_trunc, y_dev_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(emb_result)\n",
    "evaluate(emb_predict, y_dev_le, emb_history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "glove_file = 'glove.twitter.27B.' + str(GLOVE_DIMENSIONS) + 'd.txt'\n",
    "emb_dict = {}\n",
    "glove = open(Path('./db') / glove_file, encoding=\"utf8\")\n",
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    emb_dict[word] = vector\n",
    "glove.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "emb_matrix = np.zeros((vocab_size, GLOVE_DIMENSIONS))\n",
    "\n",
    "for w, i in tk.word_index.items():\n",
    "    if i < vocab_size:\n",
    "        vect = emb_dict.get(w)\n",
    "        if vect is not None:\n",
    "            emb_matrix[i] = vect\n",
    "    else:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(vocab_size, GLOVE_DIMENSIONS, input_length=max_length,\n",
    "                                   weights=[emb_matrix], trainable=False)\n",
    "sequence_input, preds = create_model(embedding_layer)\n",
    "\n",
    "glove_model = models.Model(sequence_input, preds)\n",
    "glove_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "glove_model.layers[0].set_weights([emb_matrix])\n",
    "glove_model.layers[0].trainable = False\n",
    "\n",
    "glove_history, glove_result, glove_predict = deep_model(glove_model, x_train_seq_trunc, y_train_categorical,\n",
    "                                                        x_dev_seq_trunc, y_dev_categorical)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(glove_result)\n",
    "evaluate(glove_predict, y_dev_le, glove_history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-81ce5fe2",
   "language": "python",
   "display_name": "PyCharm (BoW)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}