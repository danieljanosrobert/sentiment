{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at SZTAKI-HLT/hubert-base-cc and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from official.modeling import tf_utils\n",
    "from official import nlp\n",
    "from official.nlp import bert\n",
    "\n",
    "import official.nlp.optimization\n",
    "import official.nlp.bert.bert_models\n",
    "import official.nlp.bert.configs\n",
    "import official.nlp.bert.run_classifier\n",
    "import official.nlp.bert.tokenization\n",
    "import official.nlp.data.classifier_data_lib\n",
    "import official.nlp.modeling.losses\n",
    "import official.nlp.modeling.models\n",
    "import official.nlp.modeling.networks\n",
    "\n",
    "# BERT model, tokenizer\n",
    "\n",
    "from transformers import TFBertForSequenceClassification, TFAutoModel , AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\", num_labels=3, output_attentions=False, output_hidden_states=False)\n",
    "\n",
    "# Constants\n",
    "\n",
    "SHUFFLE_RANDOM_STATE = 42\n",
    "TRAIN_RANDOM_STATE = 42\n",
    "\n",
    "TEXT = 'Sentence'\n",
    "FILTER = 'Entity'\n",
    "START_TOKEN = 'START'\n",
    "TOKEN_LEN = 'LEN'\n",
    "Y_HEADER = 'LABEL'\n",
    "LABELS = {\n",
    "    \"NEG\": 0,\n",
    "    \"SEM\": 1,\n",
    "    \"POZ\": 2\n",
    "}\n",
    "\n",
    "EKEZET_VALTAS_RAGNAL = {\n",
    "    'a': 'á',\n",
    "    'e': 'é'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "# Text processing\n",
    "\n",
    "fix = 0\n",
    "\n",
    "def extend_context(text, context, left_index, right_index):\n",
    "    # TODO: Mi van, ha a neveket sem vesszuk bele?\n",
    "    global fix\n",
    "    if type(text) != list:\n",
    "        text = text.split()\n",
    "    start_size = 0\n",
    "    while start_size < 10:\n",
    "        if left_index > 0:\n",
    "            left_index -= 1\n",
    "            context.insert(0, text[left_index])\n",
    "            if len(text[left_index]) > 1:\n",
    "                start_size += 1\n",
    "        if right_index < len(text) -1:\n",
    "            right_index += 1\n",
    "            context.append(text[right_index])\n",
    "            if len(text[right_index]) > 1:\n",
    "                start_size += 1\n",
    "        if left_index <= 0 and right_index >= len(text) -1:\n",
    "            # fix += 1\n",
    "            break\n",
    "    return ' '.join(context)\n",
    "\n",
    "def contextualize(index, debug = False):\n",
    "    global fix\n",
    "    text = ' '.join([i for i in re.split(r'( - |(?![%.-])\\W)|(-e[\\n ])', dataset[TEXT][index]) if i])\n",
    "    context_start_index = int(dataset[START_TOKEN][index] - 1)\n",
    "    context_stop_index = int(context_start_index+dataset[TOKEN_LEN][index] - 1)\n",
    "\n",
    "    context_name_tokens = \\\n",
    "        [t for t in text\n",
    "                        .split()[context_start_index:context_stop_index+1]]\n",
    "\n",
    "    context_name = ' '.join(context_name_tokens)\n",
    "    context_filter = dataset[FILTER][index]\n",
    "\n",
    "    # Ha ragozunk, akkor a-bol á lesz pl Csaba -> Csabával\n",
    "    if len(context_name) >= len(context_filter) \\\n",
    "            and context_filter[len(context_filter)-1] in EKEZET_VALTAS_RAGNAL\\\n",
    "            and EKEZET_VALTAS_RAGNAL.get(context_filter[len(context_filter)-1]) == context_name[len(context_filter)-1]:\n",
    "        context_filter = context_filter[:-1] + context_name[len(context_filter)-1]\n",
    "\n",
    "    context_list = context_name_tokens\n",
    "    left_index = context_start_index\n",
    "    right_index = context_stop_index\n",
    "    context = extend_context(text, context_list, left_index, right_index)\n",
    "    return context\n",
    "\n",
    "    # if len(context_name) < len(filter):\n",
    "    #     if debug:\n",
    "    #         print(\"rovidebb\")\n",
    "    #         print(dataset['ID'][index])\n",
    "    #         print(dataset[START_TOKEN][index])\n",
    "    #         print(dataset[FILTER][index])\n",
    "    #         print(text)\n",
    "    #         print(text.split())\n",
    "    #         print(context_name_tokens)\n",
    "    #         print('-----')\n",
    "    #     fix += 1\n",
    "    # elif not context_name.startswith(filter)\\\n",
    "    #         and filter[len(filter)-1] in EKEZET_VALTAS_RAGNAL\\\n",
    "    #         and EKEZET_VALTAS_RAGNAL.get(filter[len(filter)-1]) != context_name[len(filter)-1]:\n",
    "    #\n",
    "    #     if debug:\n",
    "    #         print(EKEZET_VALTAS_RAGNAL.get(filter[len(filter)-1]))\n",
    "    #         print(context_name)\n",
    "    #         print(EKEZET_VALTAS_RAGNAL.get(filter[len(filter)-1]) == context_name[len(filter)-1])\n",
    "    #         print(\"nem ilyen izes\")\n",
    "    #         print(dataset['ID'][index])\n",
    "    #         print(dataset[START_TOKEN][index])\n",
    "    #         print(dataset[FILTER][index])\n",
    "    #         print(text)\n",
    "    #         print(text.split())\n",
    "    #         print(context_name_tokens)\n",
    "    #         print('-----')\n",
    "    #     fix += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7700 entries, 3856 to 7270\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   ID        7700 non-null   int64 \n",
      " 1   START     7700 non-null   int64 \n",
      " 2   LEN       7700 non-null   int64 \n",
      " 3   Entity    7700 non-null   object\n",
      " 4   Sentence  7700 non-null   object\n",
      " 5   URL       7700 non-null   object\n",
      " 6   LABEL     7700 non-null   int64 \n",
      "dtypes: int64(4), object(3)\n",
      "memory usage: 481.2+ KB\n",
      "7700\n",
      "X_train[0]:  A miskolci diákolimpiai fináléban Fejér Barna és Cs. Németh Péter fiatal tanítványai kilenc\n",
      "y_train[0]:  1\n"
     ]
    }
   ],
   "source": [
    "# Load and set up data\n",
    "\n",
    "dataset = pd.read_csv('db/train.csv', sep=';')\n",
    "dataset[Y_HEADER] = dataset[Y_HEADER].map(LABELS)\n",
    "dataset = shuffle(dataset, random_state=SHUFFLE_RANDOM_STATE)\n",
    "dataset.info()\n",
    "\n",
    "X_list = []\n",
    "print(len(dataset.index))\n",
    "for i  in range(len(dataset.index)):\n",
    "    X_list.append(contextualize(i))\n",
    "\n",
    "# print ('need fix: ', fix)\n",
    "\n",
    "X = np.asarray(X_list)\n",
    "y = dataset[Y_HEADER].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=TRAIN_RANDOM_STATE)\n",
    "\n",
    "print('X_train[0]: ', X_train[0])\n",
    "print('y_train[0]: ', y_train[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 32001\n"
     ]
    }
   ],
   "source": [
    "# Peek tokenizer\n",
    "print('Vocab size:', len(tokenizer.vocab))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['Szia', 'Világ', '!']\n",
      "ids: [15689, 6976, 8189]\n",
      "cls token: 2 sep token: 3\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a sentance\n",
    "\n",
    "tokens = tokenizer.tokenize(\"Szia Világ!\")\n",
    "print('tokens:',tokens)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print('ids:', ids)\n",
    "\n",
    "cls_token, sep_token = tokenizer.convert_tokens_to_ids(['[CLS]', '[SEP]'])\n",
    "print('cls token:', cls_token, 'sep token:', sep_token)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [],
   "source": [
    "def encode_sentence(s):\n",
    "   tokens = list(tokenizer.tokenize(s))\n",
    "   tokens.append('[SEP]')\n",
    "   return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def bert_encode(X):\n",
    "    sentence = tf.ragged.constant([\n",
    "        encode_sentence(s) for s in X])\n",
    "\n",
    "    cls = [[cls_token]]*sentence.shape[0]\n",
    "    input_word_ids = tf.concat([cls, sentence], axis=-1)\n",
    "\n",
    "    input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "\n",
    "    type_cls = tf.zeros_like(cls)\n",
    "    type_s = tf.ones_like(sentence)\n",
    "    input_type_ids = tf.concat([type_cls, type_s], axis=-1).to_tensor()\n",
    "\n",
    "    inputs = {\n",
    "      'input_word_ids': input_word_ids.to_tensor(),\n",
    "      'input_mask': input_mask,\n",
    "      'input_type_ids': input_type_ids}\n",
    "\n",
    "    return inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [
    "bert_train = bert_encode(X_train)\n",
    "bert_test = bert_encode(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_word_ids     shape: (6160, 46)\n",
      "input_mask         shape: (6160, 46)\n",
      "input_type_ids     shape: (6160, 46)\n",
      "\n",
      "train_labels       shape: (6160,)\n"
     ]
    }
   ],
   "source": [
    "for key, value in bert_train.items():\n",
    "  print(f'{key:18s} shape: {value.shape}')\n",
    "\n",
    "print(f'\\ntrain_labels {\"\":5s} shape: {y_train.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"SZTAKI-HLT/hubert-base-cc\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32001\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ha a model TFBertForSequenceClassification\n",
    "\n",
    "print(model.config)\n",
    "\n",
    "# config_dict = json.loads(tf.io.gfile.GFile('bertconfig.json').read())\n",
    "#\n",
    "# bert_config = bert.configs.BertConfig.from_dict(config_dict)\n",
    "#\n",
    "# config_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJ8AAAAeCAIAAAC9lVvKAAAABmJLR0QA/wD/AP+gvaeTAAAB8klEQVRoge1aW5aDIAyVOW7RbelCuhH3oftgPphmMlyIhNHa5uT+1NI8uYFQjiHGODiM4uvuABwXYkwf67pu23ZvKI4TMU3TQGt3nudbg3GciWVZ0sNIQ4lthwE8Ho/04H3XMpxdy3B2LcPZtQxn1zKa2A0hhBDkERX+qf56qAJOkqTSqKidkPCEIFNll9RCCDHG7MKy8f6y6Lto8M2hijbGyHOUdWmKtBMSnxAIHoujWdGlWGti9FMSw1XOdclyGiQLXAt9cUfcYOaL7HBJ7iJzKmdXlKx9xUTQe02YE4zyQnhy8ENt7VLRydXHa4eSkXX5IBU4amECSFhxfWCpZYrZzlGsSPTLjaN3IRIUGKBM0Q6GXZz/w838zFPVda2UW9b2vwS5/NsN4j6kNYJ2BMhinTtzH7SdQ2WWV/q58fSFjYuynbOXQWJX7rtZL8mKSGhyw98tiOsWHRW9YNnWeh6mUDMoJMgt85nBHGsxFJ3iobo2ITgzLcexKrstZVjsqaoR7JcqL9mEdsTT6Ppwh2yJttHpoc1GC4NqZ8YD6kWKKnlh0WvRneBr0BGegt3ubLWKKvkTOXg3OjN0hOc3kZbh7FqGs2sZzq5lOLuW4exaxu8/InqRzvHp2Pc9PfxcAvjb6saQ3l9+u4tvx4nwvmsZ3y22MALPIEWqAAAAAElFTkSuQmCC\n",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : A miskolci diákolimpiai fináléban Fejér Barna és Cs. Németh Péter fiatal tanítványai kilenc \n",
      "\n",
      "1 : Hétfőn tett tanuvallomást gyanúsítottként Gyurcsány Ferenc , az erről készült kb. fél órás \n",
      "\n",
      "0 : A kormányoldal részéről mindössze az előterjesztő ( Lázár János ) , a vezérszónok ( Szijjártó Péter ) és két \n",
      "\n",
      "1 : Gáspár Győző felesége , Bea asszony számára legfontosabb a család , viszi a háztartást és szerinte \n",
      "\n",
      "1 : A szerződést Vecsei Miklós , a Magyar Máltai Szeretetszolgálat alelnöke és Aknai Zoltán , a Menhely Alapítvány \n",
      "\n",
      "1 : ki szerdán a Kulturális Örökségvédelmi Hivatal ( KÖH ) elnökévé Réthelyi Miklós nemzeti erőforrás miniszter. \n",
      "\n",
      "1 : Ismeretlenek betörtek Tőkés László budapesti , VI. kerületi irodájába. \n",
      "\n",
      "1 : a kormány - mondta az InfoRádiónak Bod Péter Ákos az Adózás Európában elnevezésű nemzetközi konferencián. \n",
      "\n",
      "1 : és az éppen csak a napi étkezésre elég - közölte a kormányzó szóvivője , Rudi Iriawan. \n",
      "\n",
      "0 : Javier Morena egy szegény család legidősebb gyermeke volt , gyümölcsárusításból próbálták fenntartani magukat. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(10, 3), dtype=float32, numpy=\narray([[ 0.32845503,  0.07863181, -0.20424253],\n       [ 0.08015388, -0.44066608, -0.18912083],\n       [ 0.21136162, -0.26187128, -0.01093509],\n       [-0.15032732, -0.284038  ,  0.00367391],\n       [ 0.01367137, -0.30101752,  0.07181562],\n       [ 0.11750799, -0.55502206, -0.02448407],\n       [ 0.02691292, -0.36814243,  0.01793264],\n       [ 0.2017272 , -0.3457285 , -0.0309227 ],\n       [ 0.10670129, -0.5114079 , -0.41897744],\n       [-0.16983794, -0.17508838, -0.05690034]], dtype=float32)>, hidden_states=None, attentions=None)"
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch = {key: val[:10] for key, val in bert_train.items()}\n",
    "for i in range(10):\n",
    "    print(y_train[i], ':', X_train[i], '\\n')\n",
    "\n",
    "model(\n",
    "    X_batch['input_word_ids']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  110618112 \n",
      "_________________________________________________________________\n",
      "dropout_413 (Dropout)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  2307      \n",
      "=================================================================\n",
      "Total params: 110,620,419\n",
      "Trainable params: 110,620,419\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "0 <transformers.models.bert.modeling_tf_bert.TFBertMainLayer object at 0x000002230CEA5EB0>\n",
      "   no activation attribute\n",
      "1 <keras.layers.core.Dropout object at 0x000002230A6E4EB0>\n",
      "   no activation attribute\n",
      "2 <keras.layers.core.Dense object at 0x00000222BB57DCA0>\n",
      "     <function linear at 0x00000222AFE9BAF0>\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n",
    "for i, layer in enumerate (model.layers):\n",
    "    print (i, layer)\n",
    "    try:\n",
    "        print (\"    \",layer.activation)\n",
    "    except AttributeError:\n",
    "        print('   no activation attribute')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at SZTAKI-HLT/hubert-base-cc were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at SZTAKI-HLT/hubert-base-cc.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"SZTAKI-HLT/hubert-base-cc\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32001\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ha a model TFAutoModel\n",
    "\n",
    "model = TFAutoModel.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")\n",
    "\n",
    "print(model.config)\n",
    "\n",
    "# config_dict = json.loads(tf.io.gfile.GFile('bertconfig.json').read())\n",
    "#\n",
    "# bert_config = bert.configs.BertConfig.from_dict(config_dict)\n",
    "#\n",
    "# config_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAE8AAAAeCAYAAABt5kPUAAAABmJLR0QA/wD/AP+gvaeTAAABXUlEQVRoge2a0bGEIAxFkzeWYSFaiG3ZCIVIHxbC+8LJMkgCURlmcn52VUwuVwjrKoYQAhgtuL/eCkZmil+893CeZ08tQ7CuK8zzDAAA18jb972boFHw3sNxHNf2RA9u2/a5oJGxmqfAzFNg5ikw8xSYeQqqzUNEQETx/q8p6ZBorOkHax4NhIgQQoDcHV3LXd4bZpd0cBpL/csxlQ7GzuUM5M6hbeI5dyM2jZdrd9cmzSOJX9ItbQ/AjLwYhF4NLnBsm4qhorh46bHUeDpC0jzSiybRzvHJgtGzFr6Zuzhtn6Lnv15v5haZJ615dMjfTbk0Rql25T7TqcVNbdquVNdyMTlY82qvXM1KXLu/JQ/dJ6nXNTRPW8mK2CPWl/mazXuyc1/XxKfy2e2ZAjNPgZmnwMxTYOYpMPMU/PxUcc710jEE3ntYluXaxvi6hT30lkEeervLPKMae1dFwz+MoPV6C28VlQAAAABJRU5ErkJggg==\n",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : A miskolci diákolimpiai fináléban Fejér Barna és Cs. Németh Péter fiatal tanítványai kilenc \n",
      "\n",
      "1 : Hétfőn tett tanuvallomást gyanúsítottként Gyurcsány Ferenc , az erről készült kb. fél órás \n",
      "\n",
      "0 : A kormányoldal részéről mindössze az előterjesztő ( Lázár János ) , a vezérszónok ( Szijjártó Péter ) és két \n",
      "\n",
      "1 : Gáspár Győző felesége , Bea asszony számára legfontosabb a család , viszi a háztartást és szerinte \n",
      "\n",
      "1 : A szerződést Vecsei Miklós , a Magyar Máltai Szeretetszolgálat alelnöke és Aknai Zoltán , a Menhely Alapítvány \n",
      "\n",
      "1 : ki szerdán a Kulturális Örökségvédelmi Hivatal ( KÖH ) elnökévé Réthelyi Miklós nemzeti erőforrás miniszter. \n",
      "\n",
      "1 : Ismeretlenek betörtek Tőkés László budapesti , VI. kerületi irodájába. \n",
      "\n",
      "1 : a kormány - mondta az InfoRádiónak Bod Péter Ákos az Adózás Európában elnevezésű nemzetközi konferencián. \n",
      "\n",
      "1 : és az éppen csak a napi étkezésre elég - közölte a kormányzó szóvivője , Rudi Iriawan. \n",
      "\n",
      "0 : Javier Morena egy szegény család legidősebb gyermeke volt , gyümölcsárusításból próbálták fenntartani magukat. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "TFBaseModelOutputWithPooling(last_hidden_state=<tf.Tensor: shape=(10, 46, 768), dtype=float32, numpy=\narray([[[ 8.53495657e-01, -1.59622639e-01,  7.50690877e-01, ...,\n         -8.64670396e-01, -3.87077928e-01,  1.39135730e+00],\n        [-2.21885860e-01,  3.02363753e-01, -4.67386395e-01, ...,\n          9.62088257e-02, -1.54633909e-01,  6.96720183e-03],\n        [-1.26337215e-01, -5.24650931e-01, -4.10777330e-01, ...,\n         -4.55705345e-01,  1.59798861e-01,  4.36524093e-01],\n        ...,\n        [ 2.92310238e-01,  4.05387521e-01,  4.38325763e-01, ...,\n         -2.59235084e-01, -1.19538799e-01,  1.40259922e-01],\n        [ 3.49868774e-01,  3.38483542e-01,  3.77705902e-01, ...,\n         -2.63183296e-01, -6.03891760e-02,  1.37556344e-01],\n        [ 4.29671288e-01,  2.97923118e-01,  3.64649624e-01, ...,\n         -2.38864273e-01, -3.28157097e-02,  1.32595718e-01]],\n\n       [[ 1.31340015e+00, -3.32418203e-01,  5.54044902e-01, ...,\n         -6.51495636e-01, -1.00764446e-02,  6.66303694e-01],\n        [-1.72716260e-01, -1.05121545e-02,  1.56903982e-01, ...,\n         -1.70543298e-01,  5.45286499e-02, -1.03873610e-02],\n        [ 4.07294840e-01, -1.64166652e-02, -8.33330750e-02, ...,\n         -3.21678251e-01,  5.48265100e-01,  1.89062297e-01],\n        ...,\n        [ 4.81435984e-01,  3.01899582e-01,  2.91971982e-01, ...,\n         -3.44216257e-01, -1.17998734e-01, -7.90997595e-02],\n        [ 5.12349546e-01,  2.77684420e-01,  2.30695784e-01, ...,\n         -3.54075372e-01, -4.80562896e-02, -7.82000273e-02],\n        [ 5.47452569e-01,  2.57436395e-01,  2.06816956e-01, ...,\n         -3.60172004e-01, -1.69562101e-02, -7.32951164e-02]],\n\n       [[ 1.44112015e+00,  1.15817323e-01,  6.84203923e-01, ...,\n         -6.66468918e-01, -2.40201265e-01,  7.11016476e-01],\n        [ 1.52271986e-01, -9.41016749e-02, -2.12448388e-01, ...,\n          1.69185176e-02, -1.78809881e-01,  1.55631810e-01],\n        [ 1.76210910e-01, -2.48891130e-01, -2.53739297e-01, ...,\n         -3.19032788e-01,  4.06884789e-01,  4.96934533e-01],\n        ...,\n        [ 5.62443137e-01,  4.03822750e-01,  1.81034714e-01, ...,\n         -4.33594167e-01, -2.95086578e-02, -1.67548105e-01],\n        [ 5.70486903e-01,  3.67121696e-01,  1.45702600e-01, ...,\n         -4.29538339e-01,  2.91003436e-02, -1.56523660e-01],\n        [ 5.83523273e-01,  3.35080802e-01,  1.27324149e-01, ...,\n         -4.14859235e-01,  4.33782451e-02, -1.31860375e-01]],\n\n       ...,\n\n       [[ 2.65965909e-01,  4.92077440e-01,  1.21504176e+00, ...,\n         -4.28190947e-01,  3.23566675e-01, -1.49785697e-01],\n        [-3.22914869e-02,  1.00266375e-01, -1.55189559e-01, ...,\n         -3.43354642e-01, -1.24119908e-01,  2.46060774e-01],\n        [ 1.72222853e-02,  7.53230602e-03, -1.90377593e-01, ...,\n         -5.51546693e-01,  3.95770848e-01,  2.07955450e-01],\n        ...,\n        [ 3.67216796e-01,  2.52396733e-01,  3.95593733e-01, ...,\n         -2.82983869e-01,  3.60172167e-02,  5.55284843e-02],\n        [ 4.15790260e-01,  2.15467185e-01,  3.79697621e-01, ...,\n         -2.89431661e-01,  8.60646516e-02,  6.13171533e-02],\n        [ 4.66286391e-01,  1.84901237e-01,  4.05452251e-01, ...,\n         -2.69668549e-01,  9.54498351e-02,  7.80708045e-02]],\n\n       [[ 1.61346364e+00,  3.42392802e-01,  1.07733619e+00, ...,\n         -9.67084020e-02, -1.89956427e-01,  1.92865446e-01],\n        [ 9.93364155e-02, -6.89980760e-02,  3.43384176e-01, ...,\n         -7.22515434e-02, -2.69061685e-01,  1.24009766e-01],\n        [ 1.28311664e-01,  1.55140966e-01, -3.19248855e-01, ...,\n          3.45152825e-01,  3.41442704e-01,  9.57531556e-02],\n        ...,\n        [ 5.07870138e-01,  3.27615201e-01,  3.96811724e-01, ...,\n         -1.80202484e-01,  1.75875798e-01,  2.44257629e-01],\n        [ 5.76483846e-01,  2.84388840e-01,  3.58638078e-01, ...,\n         -1.68071628e-01,  2.25963026e-01,  2.41103947e-01],\n        [ 6.33242488e-01,  2.59510726e-01,  3.56190801e-01, ...,\n         -1.40081078e-01,  2.28446499e-01,  2.43722528e-01]],\n\n       [[ 1.66425300e+00,  1.47036865e-01,  6.21576548e-01, ...,\n         -7.84955919e-02, -2.61724234e-01, -1.54897571e-04],\n        [ 4.99189913e-01,  2.81494319e-01,  2.30274931e-01, ...,\n         -7.48340011e-01, -2.09711324e-02,  1.44790068e-01],\n        [ 3.66834641e-01,  6.05973840e-01, -9.79168341e-03, ...,\n         -7.17390358e-01, -1.31833442e-02,  3.54482651e-01],\n        ...,\n        [ 3.06711018e-01,  3.50848377e-01,  3.95644337e-01, ...,\n         -4.40232247e-01,  1.24846116e-01,  7.75137469e-02],\n        [ 4.13249522e-01,  2.63216764e-01,  3.39033753e-01, ...,\n         -3.97591084e-01,  1.92401230e-01,  7.55018815e-02],\n        [ 5.09675741e-01,  2.15884268e-01,  3.46420795e-01, ...,\n         -3.37836415e-01,  1.91037893e-01,  1.25089586e-01]]],\n      dtype=float32)>, pooler_output=<tf.Tensor: shape=(10, 768), dtype=float32, numpy=\narray([[ 0.754223  ,  0.90536636,  0.79754263, ...,  0.94250816,\n         0.10148366,  0.6362616 ],\n       [ 0.95484895,  0.940635  ,  0.8237963 , ...,  0.95447713,\n         0.27570575,  0.50827175],\n       [ 0.9457335 ,  0.90861297,  0.76105124, ...,  0.93447804,\n        -0.6351499 ,  0.09129633],\n       ...,\n       [ 0.82458806,  0.9266983 ,  0.75258714, ...,  0.92343223,\n        -0.75607437,  0.00913043],\n       [ 0.8872699 ,  0.8431814 ,  0.6336429 , ...,  0.8324658 ,\n        -0.867145  , -0.5890903 ],\n       [ 0.88389534,  0.92039555,  0.7509066 , ...,  0.8932317 ,\n         0.11399418, -0.72566164]], dtype=float32)>, hidden_states=None, attentions=None)"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch = {key: val[:10] for key, val in bert_train.items()}\n",
    "for i in range(10):\n",
    "    print(y_train[i], ':', X_train[i], '\\n')\n",
    "\n",
    "model(\n",
    "    X_batch['input_word_ids']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  110618112 \n",
      "=================================================================\n",
      "Total params: 110,618,112\n",
      "Trainable params: 110,618,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "0 <transformers.models.bert.modeling_tf_bert.TFBertMainLayer object at 0x0000022311E7C730>\n",
      "   no activation attribute\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n",
    "for i, layer in enumerate (model.layers):\n",
    "    print (i, layer)\n",
    "    try:\n",
    "        print (\"    \",layer.activation)\n",
    "    except AttributeError:\n",
    "        print('   no activation attribute')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}